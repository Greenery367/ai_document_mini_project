import os
import io
import json
import numpy as np
import cv2
import streamlit as st
import torch
import folium
import re
from typing import Optional
from datetime import datetime
from PIL import Image
from PIL.ExifTags import TAGS, GPSTAGS
from geopy.geocoders import Nominatim
from streamlit_folium import st_folium

# AI ëª¨ë¸ ê´€ë ¨ ë¼ì´ë¸ŒëŸ¬ë¦¬
from paddleocr import PaddleOCR
from sqlmodel import Field, Session, SQLModel, create_engine, select
from transformers import (
    AutoProcessor, AutoModelForImageClassification, 
    AutoTokenizer, AutoModelForSeq2SeqLM,
    DetrImageProcessor, DetrForObjectDetection
)
from sentence_transformers import SentenceTransformer
from kiwipiepy import Kiwi

# í™˜ê²½ ì„¤ì •
os.environ['KMP_DUPLICATE_LIB_OK'] = 'True'

# ---------------------------------------------------------
# 1. DB ëª¨ë¸ ë° ì´ˆê¸°í™”
# ---------------------------------------------------------
class Document(SQLModel, table=True):
    __table_args__ = {"extend_existing": True} 
    
    id: Optional[int] = Field(default=None, primary_key=True)
    filename: str
    doc_type: str 
    content: str 
    summary: str
    keywords: str
    structured_data: str 
    upload_date: datetime = Field(default_factory=datetime.now)
    image_data: bytes
    embedding: Optional[str] = None

engine = create_engine("sqlite:///archive.db")
SQLModel.metadata.create_all(engine)
kiwi = Kiwi()

# ---------------------------------------------------------
# 2. AI ëª¨ë¸ ë¡œë”© (ìºì‹±)
# ---------------------------------------------------------
@st.cache_resource
def load_all_models():
    # PaddleOCR ê¸°ë³¸ ì„¤ì • (ì•ˆì „í•œ íŒŒë¼ë¯¸í„°ë§Œ ì‚¬ìš©)
    ocr = PaddleOCR(
        use_angle_cls=True,
        lang='korean'
    )
    dit_p = AutoProcessor.from_pretrained("microsoft/dit-base-finetuned-rvlcdip")
    dit_m = AutoModelForImageClassification.from_pretrained("microsoft/dit-base-finetuned-rvlcdip")
    obj_p = DetrImageProcessor.from_pretrained("facebook/detr-resnet-50")
    obj_m = DetrForObjectDetection.from_pretrained("facebook/detr-resnet-50")
    sum_t = AutoTokenizer.from_pretrained("gogamza/kobart-summarization")
    sum_m = AutoModelForSeq2SeqLM.from_pretrained("gogamza/kobart-summarization")
    emb_m = SentenceTransformer("jhgan/ko-sroberta-multitask")
    return (dit_p, dit_m, ocr, obj_p, obj_m, sum_t, sum_m, emb_m)

# ---------------------------------------------------------
# 3. ë³´ì¡° ë¶„ì„ í•¨ìˆ˜
# ---------------------------------------------------------
def get_text_from_ocr(ocr_result):
    """PaddleOCR ê²°ê³¼ ë¦¬ìŠ¤íŠ¸ì—ì„œ í…ìŠ¤íŠ¸ë§Œ ì¶”ì¶œí•˜ëŠ” ì•ˆì „í•œ í•¨ìˆ˜"""
    try:
        if not ocr_result:
            print("[DEBUG] OCR ê²°ê³¼ê°€ None ë˜ëŠ” ë¹ˆ ê°’ì…ë‹ˆë‹¤.")
            return ""
        if not ocr_result[0]:
            print("[DEBUG] OCR ê²°ê³¼[0]ì´ Noneì…ë‹ˆë‹¤.")
            return ""
        
        text_list = []
        for idx, line in enumerate(ocr_result[0]):
            if line and len(line) >= 2 and line[1]:
                text_list.append(line[1][0])
        
        result = " ".join(text_list)
        print(f"[DEBUG] OCR ì¶”ì¶œ ì™„ë£Œ: {len(result)}ê¸€ì, ë¼ì¸ ìˆ˜: {len(text_list)}")
        return result
    except Exception as e:
        print(f"[DEBUG] OCR íŒŒì‹± ì—ëŸ¬: {str(e)}")
        return ""

def extract_photo_metadata(image):
    metadata = {
        'width': image.width, 'height': image.height,
        'camera_model': 'ì •ë³´ ì—†ìŒ', 'taken_date': 'ì •ë³´ ì—†ìŒ', 
        'location_address': 'ì •ë³´ ì—†ìŒ', 'lat': None, 'lng': None
    }
    try:
        exif_data = image._getexif()
        if exif_data:
            for tag_id, value in exif_data.items():
                tag = TAGS.get(tag_id, tag_id)
                if tag == "Model": metadata['camera_model'] = str(value).strip()
                elif tag in ["DateTime", "DateTimeOriginal"]: 
                    metadata['taken_date'] = str(value).replace(':', '-', 2)
                elif tag == "GPSInfo" and isinstance(value, dict):
                    gps_data = {GPSTAGS.get(t, t): value[t] for t in value}
                    if 'GPSLatitude' in gps_data and 'GPSLongitude' in gps_data:
                        def to_decimal(dms, ref):
                            d, m, s = [float(x) for x in dms]
                            res = d + m/60.0 + s/3600.0
                            return -res if ref in ['S', 'W'] else res
                        metadata['lat'] = to_decimal(gps_data['GPSLatitude'], gps_data['GPSLatitudeRef'])
                        metadata['lng'] = to_decimal(gps_data['GPSLongitude'], gps_data['GPSLongitudeRef'])
                        try:
                            geolocator = Nominatim(user_agent="geo_archive_v4")
                            loc = geolocator.reverse(f"{metadata['lat']}, {metadata['lng']}", language='ko')
                            if loc: metadata['location_address'] = loc.address
                        except: pass
    except: pass
    return metadata

def detect_photo_objects(image, processor, model):
    try:
        inputs = processor(images=image, return_tensors="pt")
        outputs = model(**inputs)
        target_sizes = torch.tensor([image.size[::-1]])
        results = processor.post_process_object_detection(outputs, target_sizes=target_sizes, threshold=0.7)[0]
        objs = [model.config.id2label[label.item()] for label in results["labels"]]
        return list(set(objs))
    except: return []

def generate_photo_keywords(metadata, objects):
    kws = ["ì‚¬ì§„"] + objects
    if metadata['camera_model'] != 'ì •ë³´ ì—†ìŒ': kws.append(metadata['camera_model'])
    if metadata['location_address'] != 'ì •ë³´ ì—†ìŒ':
        kws.extend([x.strip() for x in metadata['location_address'].split(',')[:2]])
    return ", ".join(list(dict.fromkeys(kws)))

# ---------------------------------------------------------
# 4. ë©”ì¸ í”„ë¡œì„¸ì‹± í•¨ìˆ˜ (í†µí•© ë¶„ì„)
# ---------------------------------------------------------
def process_document(uploaded_file, models):
    (dit_p, dit_m, ocr, obj_p, obj_m, sum_t, sum_m, emb_m) = models
    file_bytes = uploaded_file.read()
    raw_img = Image.open(io.BytesIO(file_bytes))
    orig_img = raw_img.convert("RGB")
    
    # 1. ë¬¸ì„œ ë¶„ë¥˜ (ì›ë³¸ ì‚¬ì´ì¦ˆ ê¸°ë°˜)
    inputs = dit_p(images=orig_img, return_tensors="pt")
    label = dit_m.config.id2label[dit_m(**inputs).logits.argmax(-1).item()].lower()
    
    # ê¸°ì´ˆ ì´ë¯¸ì§€ ë³€í™˜ (OpenCV í¬ë§·)
    img_cv = cv2.cvtColor(np.array(orig_img), cv2.COLOR_RGB2BGR)
    
    # 1ì°¨ OCR (ë¶„ë¥˜ ë³´ì¡°ìš©)
    print(f"[DEBUG] ì´ë¯¸ì§€ í¬ê¸°: {img_cv.shape}")
    ocr_res_init = ocr.ocr(img_cv)
    print(f"[DEBUG] ì´ˆê¸° OCR ê²°ê³¼ íƒ€ì…: {type(ocr_res_init)}")
    initial_text = get_text_from_ocr(ocr_res_init)
    print(f"[DEBUG] ì´ˆê¸° í…ìŠ¤íŠ¸: '{initial_text[:100] if initial_text else '(ì—†ìŒ)'}'...")

    # ë¬¸ì„œ/ì‚¬ì§„ íŒë³„ (ê°œì„ ëœ ë¡œì§)
    doc_keywords = ['receipt', 'invoice', 'form', 'letter', 'advertisement', 'resume', 'news', 'scientific', 'publication', 'memo']
    is_doc = any(x in label for x in doc_keywords) or len(initial_text) > 30
    
    # ì¶”ê°€ íŒë³„: í…ìŠ¤íŠ¸ ë°€ë„ ê³„ì‚° (ì‹ ë¬¸, ë¬¸ì„œëŠ” í…ìŠ¤íŠ¸ê°€ ë§ìŒ)
    if not is_doc and len(initial_text) > 15:
        # ì´ë¯¸ì§€ ë©´ì  ëŒ€ë¹„ í…ìŠ¤íŠ¸ ê¸¸ì´ ë¹„ìœ¨ë¡œ íŒë‹¨
        img_area = img_cv.shape[0] * img_cv.shape[1]
        text_density = len(initial_text) / (img_area / 10000)  # ë§Œ í”½ì…€ë‹¹ ë¬¸ì ìˆ˜
        if text_density > 2.0:  # ë¬¸ì ë°€ë„ê°€ ë†’ìœ¼ë©´ ë¬¸ì„œë¡œ íŒì •
            is_doc = True
    
    if is_doc:
        doc_type = "Document"
        # --- [ì ê·¹ì ì¸ í•´ìƒë„ í–¥ìƒ] ---
        height, width = img_cv.shape[:2]
        
        print(f"[DEBUG] ì›ë³¸ ì´ë¯¸ì§€ í¬ê¸°: {width}x{height}")
        
        # í•´ìƒë„ê°€ ë„ˆë¬´ ë‚®ìœ¼ë©´ ëŒ€í­ í™•ëŒ€ (ìµœì†Œ 1500px ë³´ì¥)
        target_width = 1500
        if width < target_width:
            scale = target_width / width
            new_width = int(width * scale)
            new_height = int(height * scale)
            print(f"[DEBUG] ì´ë¯¸ì§€ í™•ëŒ€: {scale:.2f}ë°° -> {new_width}x{new_height}")
            img_cv_enlarged = cv2.resize(img_cv, (new_width, new_height), interpolation=cv2.INTER_CUBIC)
        else:
            img_cv_enlarged = img_cv.copy()
        
        # í™•ëŒ€ëœ ì´ë¯¸ì§€ë¡œ OCR
        print(f"[DEBUG] OCR ì‹œì‘ (í™•ëŒ€ëœ ì´ë¯¸ì§€)")
        ocr_res_enlarged = ocr.ocr(img_cv_enlarged)
        text_enlarged = get_text_from_ocr(ocr_res_enlarged)
        
        # ì¶”ê°€ ì‹œë„: ìƒ¤í”„ë‹ ì ìš©
        kernel_sharpen = np.array([[-1,-1,-1], [-1,9,-1], [-1,-1,-1]])
        sharpened = cv2.filter2D(img_cv_enlarged, -1, kernel_sharpen)
        ocr_res_sharp = ocr.ocr(sharpened)
        text_sharp = get_text_from_ocr(ocr_res_sharp)
        
        # ê°€ì¥ ê¸´ ê²°ê³¼ ì„ íƒ
        results = [
            (text_enlarged, "í™•ëŒ€"),
            (text_sharp, "í™•ëŒ€+ìƒ¤í”„ë‹"),
            (initial_text, "ì›ë³¸")
        ]
        full_text_raw, best_method = max(results, key=lambda x: len(x[0]))
        print(f"[DEBUG] ìµœì  ë°©ë²•: {best_method}, í…ìŠ¤íŠ¸ ê¸¸ì´: {len(full_text_raw)}")
        
        # UI í‘œì‹œìš© ì´ë¯¸ì§€
        processed_gray = cv2.cvtColor(img_cv_enlarged, cv2.COLOR_BGR2GRAY)
        input_for_ocr = img_cv_enlarged
        
        # í…ìŠ¤íŠ¸ ì •ì œ (ë…¸ì´ì¦ˆ ë¬¸ì ì œê±°)
        cleaned_text = re.sub(r'\s+[a-zA-Z]\s+', ' ', full_text_raw)  # ë‹¨ì¼ ì•ŒíŒŒë²³ë§Œ ì œê±°
        cleaned_text = re.sub(r'\s+', ' ', cleaned_text).strip()
        
        full_text = cleaned_text if len(cleaned_text) > 10 else full_text_raw
        # --- [ì „ì²˜ë¦¬ ë¡œì§ ë] ---

        # í‚¤ì›Œë“œ ì¶”ì¶œ
        kw_list = [t.form for t in kiwi.tokenize(full_text) if t.tag in ['NNG', 'NNP']]
        final_keywords = ", ".join(list(dict.fromkeys(kw_list))[:10])
        
        # ìš”ì•½ ìƒì„±
        try:
            if len(full_text) > 20:
                s_inputs = sum_t([full_text], max_length=512, return_tensors="pt", truncation=True)
                s_ids = sum_m.generate(
                    s_inputs["input_ids"], 
                    num_beams=4,
                    max_length=128,
                    min_length=10,
                    repetition_penalty=3.5,
                    no_repeat_ngram_size=2,
                    eos_token_id=sum_t.eos_token_id
                )
                final_summary = sum_t.decode(s_ids[0], skip_special_tokens=True).strip()
            else:
                final_summary = "ìš”ì•½í•  í…ìŠ¤íŠ¸ê°€ ë¶€ì¡±í•©ë‹ˆë‹¤."
        except: 
            final_summary = f"{full_text[:30]}... ë‚´ìš©ì˜ ë¬¸ì„œ"
            
        # UI ì¶œë ¥ìš© ì´ë¯¸ì§€ (RGB ë³€í™˜)
        processed_img_rgb = cv2.cvtColor(processed_gray, cv2.COLOR_GRAY2RGB)
        structured_data = {}
        
    else:
        doc_type = "Photo"
        processed_img_rgb = np.array(orig_img)
        full_text = ""
        meta = extract_photo_metadata(raw_img)
        objects = detect_photo_objects(orig_img, obj_p, obj_m)
        final_keywords = generate_photo_keywords(meta, objects)
        final_summary = f"[{meta['taken_date']}] ì´¬ì˜ ì‚¬ì§„. íƒì§€ ê°ì²´: {', '.join(objects)}"
        structured_data = {'exif': meta, 'objects': objects}

    embedding = emb_m.encode(full_text + " " + final_keywords).tolist()
    return (doc_type, full_text, final_summary, final_keywords, structured_data, file_bytes, embedding, processed_img_rgb)

# ---------------------------------------------------------
# 5. UI ë° ì§€ë„ í‘œì‹œ
# ---------------------------------------------------------
def display_photo_locations(items):
    locs = []
    for d in items:
        try:
            sd = json.loads(d.structured_data)
            if 'exif' in sd and sd['exif'].get('lat') is not None:
                locs.append({
                    'lat': sd['exif']['lat'], 
                    'lng': sd['exif']['lng'], 
                    'name': d.filename, 
                    'addr': sd['exif'].get('location_address', 'ì£¼ì†Œ ë¯¸ìƒ')
                })
        except: continue
    
    if locs:
        m = folium.Map(location=[locs[0]['lat'], locs[0]['lng']], zoom_start=12)
        for l in locs:
            folium.Marker(
                [l['lat'], l['lng']], 
                popup=folium.Popup(f"<b>{l['name']}</b><br>{l['addr']}", max_width=300),
                tooltip=l['name'],
                icon=folium.Icon(color='red', icon='camera')
            ).add_to(m)
        st_folium(m, width=1200, height=600)
    else:
        st.info("ğŸ“ ì§€ë„ì— í‘œì‹œí•  ìœ„ì¹˜ ì •ë³´(GPS)ê°€ í¬í•¨ëœ ì‚¬ì§„ì´ ì—†ìŠµë‹ˆë‹¤.")

# ë©”ì¸ ì‹¤í–‰ë¶€
st.set_page_config(layout="wide", page_title="AI Multi-Archive")
st.title("ğŸŒŸ ë©€í‹°ëª¨ë‹¬ AI í†µí•© ì•„ì¹´ì´ë¸Œ")

models = load_all_models()
t1, t2, t3, t4 = st.tabs(["ğŸ“¤ ì—…ë¡œë“œ", "ğŸ” ê²€ìƒ‰", "ğŸ“ ì•„ì¹´ì´ë¸Œ", "ğŸ“ ì§€ë„"])

with t1:
    file = st.file_uploader("ì´ë¯¸ì§€ ì—…ë¡œë“œ", type=['jpg', 'png', 'jpeg'])
    if file:
        if "res" not in st.session_state or st.session_state.get("fname") != file.name:
            with st.spinner("ë¶„ì„ ì¤‘..."):
                st.session_state.res = process_document(file, models)
                st.session_state.fname = file.name
        
        r = st.session_state.res
        
        # ë””ë²„ê·¸ìš© ë³€ìˆ˜ ì €ì¥ (ì„¸ì…˜ ìƒíƒœì— ì¶”ê°€ ì •ë³´ ì €ì¥)
        if "debug_info" not in st.session_state:
            st.session_state.debug_info = {}
        
        col1, col2 = st.columns(2)
        
        # ì›ë³¸ ì´ë¯¸ì§€ í‘œì‹œ
        orig_display = Image.open(io.BytesIO(r[5]))
        col1.image(orig_display, caption="ì›ë³¸", use_container_width=True)
        
        # ì „ì²˜ë¦¬ ê²°ê³¼ í‘œì‹œ (numpy arrayë¥¼ PILë¡œ ë³€í™˜)
        if r[0] == "Document":
            col2.image(r[7], caption="ì „ì²˜ë¦¬ ê²°ê³¼", use_container_width=True)
        else:
            col2.image(orig_display, caption="ì‚¬ì§„ (ì „ì²˜ë¦¬ ì—†ìŒ)", use_container_width=True)
        
        st.write(f"**ë¶„ë¥˜:** {r[0]} | **í‚¤ì›Œë“œ:** `{r[3]}`")
        
        # OCR ë””ë²„ê·¸ ì •ë³´ ì¶”ê°€
        with st.expander("ğŸ” OCR ë””ë²„ê·¸ ì •ë³´", expanded=True):
            st.write(f"**ì¶”ì¶œëœ í…ìŠ¤íŠ¸ ê¸¸ì´:** {len(r[1])} ê¸€ì")
            
            # ì¶”ì¶œëœ í…ìŠ¤íŠ¸ í‘œì‹œ
            if r[1]:
                st.text_area("ì¶”ì¶œëœ ì „ì²´ í…ìŠ¤íŠ¸", r[1], height=150, key="ocr_text")
                st.caption(f"í‚¤ì›Œë“œ: {r[3]}")
            else:
                st.text_area("ì¶”ì¶œëœ ì „ì²´ í…ìŠ¤íŠ¸", "(í…ìŠ¤íŠ¸ ì—†ìŒ)", height=150, key="ocr_text")
            
            if not r[1] or len(r[1]) < 50:
                st.error("âš ï¸ OCR í’ˆì§ˆ ì €í•˜: í…ìŠ¤íŠ¸ë¥¼ ì œëŒ€ë¡œ ì¶”ì¶œí•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")
                st.info("""
ğŸ’¡ **ë¬¸ì œ ì§„ë‹¨ ì²´í¬ë¦¬ìŠ¤íŠ¸:**
1. **ì´ë¯¸ì§€ í•´ìƒë„**: ìµœì†Œ 1000px ì´ìƒ ê¶Œì¥ (í˜„ì¬ news.jpgëŠ” í•´ìƒë„ê°€ ë‚®ì„ ìˆ˜ ìˆìŒ)
2. **ê¸€ì í¬ê¸°**: ì‹ ë¬¸ ê¸€ì”¨ê°€ ë„ˆë¬´ ì‘ìœ¼ë©´ ì¸ì‹ ì‹¤íŒ¨
3. **PaddleOCR ì–¸ì–´íŒ©**: 'korean' ëª¨ë¸ì´ ì œëŒ€ë¡œ ë‹¤ìš´ë¡œë“œë˜ì—ˆëŠ”ì§€ í™•ì¸
4. **ì´ë¯¸ì§€ í’ˆì§ˆ**: JPG ì••ì¶•ìœ¼ë¡œ ê¸€ìê°€ íë ¤ì¡Œì„ ê°€ëŠ¥ì„±

**í•´ê²° ë°©ë²•:**
- Streamlit ì¬ì‹œì‘ í›„ ìºì‹œ í´ë¦¬ì–´ (ì¢Œì¸¡ ë©”ë‰´ > Clear cache)
- ë” ê³ í•´ìƒë„ ì´ë¯¸ì§€ë¡œ í…ŒìŠ¤íŠ¸
- PNG í¬ë§·ìœ¼ë¡œ ë³€í™˜ í›„ ì¬ì‹œë„
- í„°ë¯¸ë„ [DEBUG] ë©”ì‹œì§€ í™•ì¸
                """)
        
        st.info(f"**ìš”ì•½:** {r[2]}")
        
        if st.button("ğŸš€ ìµœì¢… ì €ì¥", type="primary"):
            with Session(engine) as session:
                new_doc = Document(filename=file.name, doc_type=r[0], content=r[1], 
                                   summary=r[2], keywords=r[3], 
                                   structured_data=json.dumps(r[4], ensure_ascii=False),
                                   image_data=r[5], embedding=json.dumps(r[6]))
                session.add(new_doc)
                session.commit()
            st.success("ì €ì¥ ì™„ë£Œ!")

with t2:
    q = st.text_input("ê²€ìƒ‰ì–´ (ê°ì²´, ì¥ì†Œ, ë‚´ìš© ë“±)")
    if q:
        with Session(engine) as session:
            results = session.exec(select(Document).where((Document.content.contains(q)) | (Document.keywords.contains(q)))).all()
            for d in results:
                with st.expander(f"ğŸ“„ {d.filename} ({d.doc_type})"):
                    sc1, sc2 = st.columns([1, 3])
                    sc1.image(d.image_data)
                    sc2.write(f"**ìš”ì•½:** {d.summary}")
                    sc2.write(f"**í‚¤ì›Œë“œ:** `{d.keywords}`")

with t3:
    with Session(engine) as session:
        items = session.exec(select(Document).order_by(Document.upload_date.desc())).all()
        for item in items:
            with st.container(border=True):
                c1, c2 = st.columns([1, 4])
                c1.image(item.image_data)
                c2.write(f"**{item.filename}** ({item.doc_type})")
                c2.caption(f"ìš”ì•½: {item.summary} | í‚¤ì›Œë“œ: {item.keywords}")
                if st.button("ğŸ—‘ï¸ ì‚­ì œ", key=f"del_{item.id}"):
                    session.delete(item); session.commit(); st.rerun()

with t4:
    st.header("ğŸ“ ì‚¬ì§„ ì´¬ì˜ ìœ„ì¹˜")
    with Session(engine) as session:
        all_docs = session.exec(select(Document)).all()
        display_photo_locations(all_docs)
